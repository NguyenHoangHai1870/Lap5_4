{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFf2sN_80OCu",
        "outputId": "97f00f7d-58fc-4840-ca0a-b70513fa2d4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=103d1877d8949fd85682fc064287141b884f6e98c3aaf59efb10331948ba10bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/b8/73/0b2c1a76b701a677653dd79ece07cfabd7457989dbfbdcd8d7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "# Cell 0: Cài đặt thư viện cần thiết\n",
        "!pip install datasets seqeval\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Import và tải dữ liệu CoNLL 2003 (bản mirror)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Đặt seed cho reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leqRZJoG0s_f",
        "outputId": "734af37d-8606-4771-ed47-ea9cb439b936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lấy tokens và ner_tags\n",
        "train_sentences = dataset[\"train\"][\"tokens\"]\n",
        "train_tags_ids = dataset[\"train\"][\"ner_tags\"]\n",
        "\n",
        "val_sentences = dataset[\"validation\"][\"tokens\"]\n",
        "val_tags_ids = dataset[\"validation\"][\"ner_tags\"]\n",
        "\n",
        "# Vì bản parquet mới không lưu tên nhãn → tự định nghĩa nhãn CoNLL2003\n",
        "tag_names = [\n",
        "    \"O\",\n",
        "    \"B-MISC\", \"I-MISC\",\n",
        "    \"B-PER\", \"I-PER\",\n",
        "    \"B-ORG\", \"I-ORG\",\n",
        "    \"B-LOC\", \"I-LOC\"\n",
        "]\n",
        "\n",
        "print(\"Tag names:\", tag_names)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXr9vFhF0v9L",
        "outputId": "31ef2190-df25-46d0-ef45-75526dd5510a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag names: ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Chuyển ner_tags (id) -> string và xây vocabulary\n",
        "\n",
        "# 1) Chuyển nhãn sang string\n",
        "train_tags_str = [[tag_names[tag_id] for tag_id in seq] for seq in train_tags_ids]\n",
        "val_tags_str   = [[tag_names[tag_id] for tag_id in seq] for seq in val_tags_ids]\n",
        "\n",
        "# 2) Xây word_to_ix từ tập train\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "UNK_TOKEN = \"<UNK>\"\n",
        "\n",
        "word_to_ix = {\n",
        "    PAD_TOKEN: 0,\n",
        "    UNK_TOKEN: 1,\n",
        "}\n",
        "\n",
        "for sent in train_sentences:\n",
        "    for w in sent:\n",
        "        if w not in word_to_ix:\n",
        "            word_to_ix[w] = len(word_to_ix)\n",
        "\n",
        "vocab_size = len(word_to_ix)\n",
        "\n",
        "# 3) Xây tag_to_ix từ danh sách tag_names\n",
        "tag_to_ix = {tag: i for i, tag in enumerate(tag_names)}\n",
        "num_tags = len(tag_to_ix)\n",
        "\n",
        "print(\"Số lượng từ (vocab_size):\", vocab_size)\n",
        "print(\"Số lượng nhãn NER:\", num_tags)\n",
        "print(\"Ví dụ word_to_ix (10 phần tử đầu):\")\n",
        "for i, (w, idx) in enumerate(word_to_ix.items()):\n",
        "    print(w, \"->\", idx)\n",
        "    if i >= 9:\n",
        "        break\n",
        "\n",
        "print(\"\\nTag_to_ix:\")\n",
        "print(tag_to_ix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO50gzgC3Ym8",
        "outputId": "571d2a08-2b76-4a4a-a846-5387570814f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Số lượng từ (vocab_size): 23625\n",
            "Số lượng nhãn NER: 9\n",
            "Ví dụ word_to_ix (10 phần tử đầu):\n",
            "<PAD> -> 0\n",
            "<UNK> -> 1\n",
            "EU -> 2\n",
            "rejects -> 3\n",
            "German -> 4\n",
            "call -> 5\n",
            "to -> 6\n",
            "boycott -> 7\n",
            "British -> 8\n",
            "lamb -> 9\n",
            "\n",
            "Tag_to_ix:\n",
            "{'O': 0, 'B-MISC': 1, 'I-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-ORG': 5, 'I-ORG': 6, 'B-LOC': 7, 'I-LOC': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Định nghĩa Dataset cho NER\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, word_to_ix, tag_to_ix):\n",
        "        \"\"\"\n",
        "        sentences: list[list[str]]\n",
        "        tags: list[list[str]]  (nhãn dạng string)\n",
        "        \"\"\"\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        words = self.sentences[idx]\n",
        "        tags = self.tags[idx]\n",
        "\n",
        "        word_ids = [\n",
        "            self.word_to_ix.get(w, self.word_to_ix[\"<UNK>\"])\n",
        "            for w in words\n",
        "        ]\n",
        "        tag_ids = [\n",
        "            self.tag_to_ix[t]\n",
        "            for t in tags\n",
        "        ]\n",
        "\n",
        "        return word_ids, tag_ids\n"
      ],
      "metadata": {
        "id": "JZizni3e3bIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 (tiếp): collate_fn để pad\n",
        "\n",
        "PAD_IDX = word_to_ix[PAD_TOKEN]\n",
        "TAG_PAD_IDX = -1  # padding cho nhãn\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    batch: list of (word_ids, tag_ids)\n",
        "    \"\"\"\n",
        "    word_seqs, tag_seqs = zip(*batch)\n",
        "\n",
        "    word_tensors = [torch.tensor(seq, dtype=torch.long) for seq in word_seqs]\n",
        "    tag_tensors  = [torch.tensor(seq, dtype=torch.long) for seq in tag_seqs]\n",
        "\n",
        "    padded_words = pad_sequence(word_tensors, batch_first=True, padding_value=PAD_IDX)\n",
        "    padded_tags  = pad_sequence(tag_tensors,  batch_first=True, padding_value=TAG_PAD_IDX)\n",
        "\n",
        "    lengths = torch.tensor([len(seq) for seq in word_seqs], dtype=torch.long)\n",
        "\n",
        "    return padded_words, padded_tags, lengths\n"
      ],
      "metadata": {
        "id": "yTLjhHPS3dKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 (tiếp): DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = NERDataset(train_sentences, train_tags_str, word_to_ix, tag_to_ix)\n",
        "val_dataset   = NERDataset(val_sentences,   val_tags_str,   word_to_ix, tag_to_ix)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Test 1 batch\n",
        "batch_words, batch_tags, batch_lengths = next(iter(train_loader))\n",
        "print(\"batch_words shape:\", batch_words.shape)\n",
        "print(\"batch_tags shape:\", batch_tags.shape)\n",
        "print(\"batch_lengths:\", batch_lengths[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tjuwbl0I3fV7",
        "outputId": "f63e1fbd-ab6b-432a-d6bb-73ee9218cd01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_words shape: torch.Size([32, 47])\n",
            "batch_tags shape: torch.Size([32, 47])\n",
            "batch_lengths: tensor([10, 10, 15,  2,  5, 23,  4, 41,  2,  4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Định nghĩa mô hình LSTM cho NER\n",
        "\n",
        "class SimpleRNNForTokenClassification(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        tagset_size,\n",
        "        embedding_dim=100,\n",
        "        hidden_dim=128,\n",
        "        num_layers=1,\n",
        "        bidirectional=True,\n",
        "        pad_idx=0\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=pad_idx\n",
        "        )\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        direction_factor = 2 if bidirectional else 1\n",
        "        self.fc = nn.Linear(hidden_dim * direction_factor, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        embeddings = self.embedding(x)     # (B, L, E)\n",
        "        outputs, _ = self.lstm(embeddings) # (B, L, H*dir)\n",
        "        logits = self.fc(outputs)          # (B, L, num_tags)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "S0RLGiKa3hMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 (tiếp): Khởi tạo model, optimizer, loss\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "num_layers = 1\n",
        "bidirectional = True\n",
        "\n",
        "model = SimpleRNNForTokenClassification(\n",
        "    vocab_size=vocab_size,\n",
        "    tagset_size=num_tags,\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    num_layers=num_layers,\n",
        "    bidirectional=bidirectional,\n",
        "    pad_idx=PAD_IDX\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TAG_PAD_IDX)\n",
        "\n",
        "model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhM1sVKU3jRh",
        "outputId": "b8655401-bb1c-4c43-dd37-9cf8c6cb693f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleRNNForTokenClassification(\n",
              "  (embedding): Embedding(23625, 100, padding_idx=0)\n",
              "  (lstm): LSTM(100, 128, batch_first=True, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: train_one_epoch\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for batch_words, batch_tags, batch_lengths in dataloader:\n",
        "        batch_words = batch_words.to(device)\n",
        "        batch_tags  = batch_tags.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(batch_words)  # (B, L, C)\n",
        "\n",
        "        B, L, C = logits.shape\n",
        "        logits_flat = logits.view(B * L, C)\n",
        "        targets_flat = batch_tags.view(B * L)\n",
        "\n",
        "        loss = criterion(logits_flat, targets_flat)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            mask = (targets_flat != TAG_PAD_IDX)\n",
        "            num_valid = mask.sum().item()\n",
        "            total_loss += loss.item() * num_valid\n",
        "            total_tokens += num_valid\n",
        "\n",
        "    avg_loss = total_loss / max(total_tokens, 1)\n",
        "    return avg_loss\n"
      ],
      "metadata": {
        "id": "Nx1y9MTJ3lRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 (tiếp): evaluate accuracy trên dev\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_words, batch_tags, batch_lengths in dataloader:\n",
        "            batch_words = batch_words.to(device)\n",
        "            batch_tags  = batch_tags.to(device)\n",
        "\n",
        "            logits = model(batch_words)       # (B, L, C)\n",
        "            preds = logits.argmax(dim=-1)     # (B, L)\n",
        "\n",
        "            mask = (batch_tags != TAG_PAD_IDX)\n",
        "\n",
        "            correct = (preds == batch_tags) & mask\n",
        "            total_correct += correct.sum().item()\n",
        "            total_tokens  += mask.sum().item()\n",
        "\n",
        "    accuracy = total_correct / max(total_tokens, 1)\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "wrDx_c9j3oHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 (tiếp): Chạy huấn luyện\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_acc = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{num_epochs} - Train loss: {train_loss:.4f} - Val accuracy: {val_acc:.4f}\")\n",
        "\n",
        "final_val_acc = evaluate(model, val_loader, device)\n",
        "print(\"\\nĐộ chính xác cuối cùng trên tập validation:\", final_val_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6b1o29z3qD3",
        "outputId": "6b0a9e2d-4248-44a4-8bfe-a18a5ec1e946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 - Train loss: 0.5909 - Val accuracy: 0.8828\n",
            "Epoch 2/3 - Train loss: 0.2670 - Val accuracy: 0.9069\n",
            "Epoch 3/3 - Train loss: 0.1523 - Val accuracy: 0.9197\n",
            "\n",
            "Độ chính xác cuối cùng trên tập validation: 0.9196877068649975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Hàm predict_sentence(sentence)\n",
        "\n",
        "ix_to_tag = {idx: tag for tag, idx in tag_to_ix.items()}\n",
        "\n",
        "def predict_sentence(model, sentence, word_to_ix, ix_to_tag, device):\n",
        "    \"\"\"\n",
        "    sentence: string\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    tokens = sentence.split()\n",
        "\n",
        "    word_ids = [word_to_ix.get(w, word_to_ix[\"<UNK>\"]) for w in tokens]\n",
        "    input_tensor = torch.tensor([word_ids], dtype=torch.long).to(device)  # (1, L)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)          # (1, L, C)\n",
        "        preds = logits.argmax(dim=-1).squeeze(0).tolist()  # (L,)\n",
        "\n",
        "    pred_tags = [ix_to_tag[p] for p in preds]\n",
        "\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Predictions:\")\n",
        "    for w, t in zip(tokens, pred_tags):\n",
        "        print(f\"{w:15s} -> {t}\")\n"
      ],
      "metadata": {
        "id": "Jy5BOyla3soq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 (tiếp): Test câu ví dụ\n",
        "\n",
        "test_sentence = \"VNU University is located in Hanoi\"\n",
        "predict_sentence(model, test_sentence, word_to_ix, ix_to_tag, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72hcyLcc3vTX",
        "outputId": "b15e23c3-e98f-4051-d77f-70dc16782b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: VNU University is located in Hanoi\n",
            "Predictions:\n",
            "VNU             -> B-PER\n",
            "University      -> I-PER\n",
            "is              -> O\n",
            "located         -> O\n",
            "in              -> O\n",
            "Hanoi           -> B-LOC\n"
          ]
        }
      ]
    }
  ]
}